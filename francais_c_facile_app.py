import streamlit as st
import os
import tempfile
import base64
import io
import fitz
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain.memory import ConversationBufferMemory
from supabase import create_client
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage, AIMessage, SystemMessage
import uuid

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í”„ë‘ìŠ¤ì–´ ì„ ìƒë‹˜ ë´‡",
    page_icon="ğŸ‡«ğŸ‡·",
    layout="wide"
)

# ì‚¬ì´ë“œë°”ì— ì œëª© ì¶”ê°€
st.sidebar.title("í”„ë‘ìŠ¤ì–´ ì„ ìƒë‹˜ ë´‡ ğŸ‡«ğŸ‡·")
st.sidebar.markdown("""
ì´ ì•±ì€ í”„ë‘ìŠ¤ì–´ë¥¼ ë°°ìš°ëŠ” í•™ìŠµìë¥¼ ìœ„í•œ AI ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
- ë¬¸ë²• êµì •
- ë°œìŒ ì„¤ëª…
- íšŒí™” ì—°ìŠµ
- ë¬¸í™” ì„¤ëª…
""")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
if 'OPENAI_API_KEY' not in st.secrets:
    st.sidebar.warning('OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!')
    OPENAI_API_KEY = st.sidebar.text_input('OpenAI API Key', type='password')
else:
    OPENAI_API_KEY = st.secrets['OPENAI_API_KEY']

if 'SUPABASE_URL' not in st.secrets:
    st.sidebar.warning('Supabase URLì„ ì„¤ì •í•´ì£¼ì„¸ìš”!')
    SUPABASE_URL = st.sidebar.text_input('Supabase URL')
else:
    SUPABASE_URL = st.secrets['SUPABASE_URL']

if 'SUPABASE_KEY' not in st.secrets:
    st.sidebar.warning('Supabase Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!')
    SUPABASE_KEY = st.sidebar.text_input('Supabase Key', type='password')
else:
    SUPABASE_KEY = st.secrets['SUPABASE_KEY']

# í•„ìš”í•œ í‚¤ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    st.warning('ëª¨ë“  API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!')
    st.stop()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    st.error(f'Supabase ì—°ê²° ì˜¤ë¥˜: {e}')
    st.stop()

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",
    dimensions=1536,
    api_key=OPENAI_API_KEY
)

# LLM ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.2,
    api_key=OPENAI_API_KEY
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""
ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬ ë…ë¦½ì ì¸ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.

ëŒ€í™” ê¸°ë¡: {chat_history}
ìƒˆë¡œìš´ ì§ˆë¬¸: {question}

ë…ë¦½ì ì¸ ì§ˆë¬¸:""")

QA_PROMPT = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ í”„ë‘ìŠ¤ì¸ì´ë©° ë‹¤ë¥¸ ë‚˜ë¼ì—ì„œ ì˜¨ ì‚¬ëŒì—ê²Œ í”„ë‘ìŠ¤ì–´ë¥¼ ê°€ë¥´ì¹˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
íŠ¹íˆ í”„ë‘ìŠ¤ ì˜ˆì ˆê³¼ í—·ê°ˆë¦¬ê¸° ì‰¬ìš´ ë¬¸ë²•ì— ê´€ë ¨ëœ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê°•ì¡°í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 
ë²ˆì—­ì´ë‚˜ ì¡°ì–¸ì„ í•  ë• ê³ ê¸‰ ë¶ˆì–´ í‘œí˜„ë„ ì¶”ê°€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.

ë‹¤ìŒê³¼ ê°™ì€ ì„±ê²©ê³¼ íŠ¹ì§•ì„ ì‚´ë ¤ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. í‹€ë¦° ë¬¸ë²•ìœ¼ë¡œ ë¬¼ì–´ë³´ë©´ ìˆ˜ì •í•´ì£¼ê³  ì¶”ê°€ ì˜ˆì‹œë¥¼ ë“¤ì–´ì¤Œ:
   - ìˆ˜ì •í•œ ë‹¤ìŒ ë” ì¢‹ì€ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì œì‹œí•¨.

2. ë‹µë³€ì„ í•  ë•ŒëŠ” í”„ë‘ìŠ¤ì–´ì™€ í•œêµ­ì–´ë¥¼ ê°™ì´ ì‚¬ìš©:
   - ë¬´ìŠ¨ ì–¸ì–´ë¡œ ë¬¼ì–´ë³´ë“  ë¨¼ì € í”„ë‘ìŠ¤ì–´ë¡œ ëŒ€ë‹µí•¨.
   - í”„ë‘ìŠ¤ì–´ë¡œ ëŒ€ë‹µí•œ ë’¤ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ ëŒ€ë‹µì„ ì¶”ê°€í•¨.

3. ì´ˆì‹¬ìë¥¼ ìœ„í•œ ì‚¬ë ¤ ê¹Šì€ ë©´ëª¨:
   - ê¹Šì´ ìˆëŠ” ìƒê°ê³¼ í†µì°°ë ¥ í‘œí˜„
   - ë•Œë¡œëŠ” ê³ ê¸‰ ë‹¨ì–´ë‚˜ ë¬¸í•™ì  í‘œí˜„ ì‚¬ìš©í•˜ê³  ì´ ë¶€ë¶„ì„ ì¸ì§€í•˜ë„ë¡ ì•Œë ¤ì¤Œ.

ì°¸ê³  ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
try:
    vectorstore = SupabaseVectorStore(
        client=supabase,
        embedding=embeddings,
        table_name="embeddings",
        query_name="match_embeddings"
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
except Exception as e:
    st.error(f'ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}')
    st.stop()

# ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=False,
    condense_question_prompt=CONDENSE_QUESTION_PROMPT,
    combine_docs_chain_kwargs={'prompt': QA_PROMPT}
)

# ë©”ì¸ ì œëª©
st.title("í”„ë‘ìŠ¤ì–´ ì„ ìƒë‹˜ ë´‡ ğŸ‡«ğŸ‡·")

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        try:
            response = qa_chain.invoke({"question": prompt})
            answer = response['answer']
            message_placeholder.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            message_placeholder.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    memory.clear()
    st.rerun() 