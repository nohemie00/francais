import streamlit as st
import json, re, ast, uuid, traceback
import numpy as np
from typing import List, Any, Optional
from supabase import create_client
from langchain.schema import Document, BaseRetriever
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain_community.retrievers import BM25Retriever
from langchain_cohere import CohereEmbeddings
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="France Curator Mme.Noy",
    page_icon="ğŸ‡«ğŸ‡·",
    layout="wide"
)

# ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• (ë°°ê²½ íŒŒë‘ + ê¸€ì í°ìƒ‰)
st.markdown("""
    <style>
    .stApp {
        background-color: #0047AB;
        color: white;
    }
    .stMarkdown p, .stTextInput > div > div > input {
        color: white !important;
    }
    .stTextInput > div > div {
        background-color: #0055cc !important;
    }
    .stChatMessage {
        background-color: #0055cc;
        color: white;
        border-radius: 10px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .stChatInputContainer {
        background-color: #003b7a;
        padding: 1rem;
        border-radius: 10px;
    }
    button[kind="primary"] {
        background-color: #FF4B4B;
        color: white;
    }
    </style>
""", unsafe_allow_html=True)

# ì»¤ë²„ ì´ë¯¸ì§€
st.image("https://raw.githubusercontent.com/nohemie00/francais/main/assets/FRANCAIS_.png", use_container_width=True)

# ì‚¬ì´ë“œë°” ë¬¸ì¥ ìƒ‰ìƒ ì»¤ìŠ¤í„°ë§ˆì´ì§•
st.markdown("""
    <style>
    .css-1d391kg p.sidebar-highlight {
        color: #B8D8FF !important;
        font-size: 16px;
        line-height: 1.6;
    }
    </style>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” ë‚´ìš©
with st.sidebar:
    st.markdown("<h2 style='color:#4F8BF9;'>ğŸ§‘â€ğŸ« Curator AI: FR</h2>", unsafe_allow_html=True)
    st.markdown("""
    - í”„ë‘ìŠ¤ì–´ ë¬¸ë²•/íšŒí™”/ê³ ê¸‰í‘œí˜„
    - í”„ë‘ìŠ¤ ë°•ë¬¼ê´€ íë ˆì´ì…˜
    - í”„ë‘ìŠ¤ ë¬¸í™”/ì—­ì‚¬/ì˜ˆìˆ 
    - í”„ë‘ìŠ¤ì— ëŒ€í•œ ëª¨ë“  ê²ƒ
    """)
    if st.button("ğŸ’¬ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []


# --- ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨ ---
if not all([OPENAI_API_KEY, COHERE_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY]):
    st.warning("â— ëª¨ë“  API í‚¤ì™€ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ---
@st.cache_resource(show_spinner=False)
def init():
    client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)
    embeddings = CohereEmbeddings(model="embed-v4.0", cohere_api_key=COHERE_API_KEY)
    llm = ChatOpenAI(model_name="gpt-4", temperature=0.2, api_key=OPENAI_API_KEY)

    text_resp = client.table("text_embeddings").select("content,metadata").limit(5000).execute()
    text_docs = [Document(page_content=it["content"], metadata=it.get("metadata", {})) for it in text_resp.data]
    texts = [d.page_content for d in text_docs]
    bm25 = BM25Retriever.from_texts(texts=texts, metadatas=[d.metadata for d in text_docs], k=5)

    class SupabaseRetriever:
        def __init__(self, table_name="text_embeddings", query_name="match_text_embeddings", k=5):
            self.table = table_name
            self.query = query_name
            self.k = k
        def invoke(self, query, page_filter=None):
            try:
                q_emb = embeddings.embed_query(query)
                matches = client.rpc(self.query, {
                    "query_embedding": q_emb,
                    "match_threshold": 0.5,
                    "match_count": self.k
                }).execute()
                docs = []
                for m in matches.data:
                    meta = m.get("metadata", {}) or {}
                    meta["similarity"] = float(m.get("similarity", 0))
                    meta["source"] = "Vector"
                    if page_filter and str(meta.get("page", "")) != str(page_filter):
                        continue
                    docs.append(Document(page_content=m["content"], metadata=meta))
                return docs
            except Exception as e:
                print("Vector search error:", e)
                return []

        def get_relevant_documents(self, query):
            return self.invoke(query)

    vector_retriever = SupabaseRetriever()

from pydantic import Field
from langchain_core.retrievers import BaseRetriever

class EnsembleRetriever(BaseRetriever):
    retrievers: List[Any] = Field(...)
    weights: List[float] = Field(...)
    
    retriever_names = ["BM25", "Vector"]

    class Config:
        arbitrary_types_allowed = True

    def _get_relevant_documents(self, query):
        all_docs = []
        for i, r in enumerate(self.retrievers):
            try:
                docs = r.get_relevant_documents(query)
                for j, d in enumerate(docs):
                    d.metadata = d.metadata or {}
                    d.metadata.update({
                        "source": self.retriever_names[i],
                        "rank": j,
                        "score": 1 / (1 + j) * self.weights[i]
                    })
                    all_docs.append(d)
            except:
                pass
        seen, final = set(), []
        for d in sorted(all_docs, key=lambda x: x.metadata.get("score", 0), reverse=True):
            h = hash(d.page_content)
            if h not in seen:
                seen.add(h)
                final.append(d)
            if len(final) >= 5:
                break
        return final
        
# ì•„ë˜ëŠ” ë°˜ë“œì‹œ í•¨ìˆ˜ë¡œ ê°ì‹¸ì•¼ í•¨!
def init():
    # ì´ ì•ˆì—ì„œ í•„ìš”í•œ ì„¤ì •ë“¤ ì§„í–‰
    client = ...
    embeddings = ...
    llm = ...
    bm25 = ...
    vector_retriever = SupabaseRetriever()

    hybrid = EnsembleRetriever(retrievers=[bm25, vector_retriever], weights=[0.3, 0.7])

    return client, embeddings, llm, hybrid

client, embeddings, llm, hybrid_retriever = init()

# --- ì´ë¯¸ì§€ ê´€ë ¨ í•¨ìˆ˜ ---
def analyze_image_relevance(image_url: str, query: str) -> float:
    try:
        q_emb = embeddings.embed_query(query)
        resp = client.table("image_embeddings").select("embedding").eq("metadata->>image_url", image_url).execute()
        if not resp.data: return 0.5
        emb = ast.literal_eval(resp.data[0]["embedding"])
        sim = np.dot(q_emb, emb) / (np.linalg.norm(q_emb) * np.linalg.norm(emb))
        return (sim + 1) / 2
    except:
        return 0.5

def get_best_images(pages: List[str], query: str, k=3):
    imgs = []
    for p in pages[:3]:
        resp = client.table("image_embeddings").select("metadata").eq("metadata->>page", str(p)).execute()
        for it in resp.data:
            url = it["metadata"].get("image_url")
            if not url: continue
            score = analyze_image_relevance(url, query)
            imgs.append({"url": url, "page": p, "score": score})
    imgs.sort(key=lambda x: x["score"], reverse=True)
    return imgs[:k]

# --- í”„ë¡¬í”„íŠ¸ ---
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(
"""ë‹¤ìŒ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ ë…ë¦½ì ì¸ í”„ë‘ìŠ¤ì–´ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì¤˜:
{chat_history}
ìƒˆ ì§ˆë¬¸: {question}
â‡’""")

QA_PROMPT = PromptTemplate.from_template(
"""ë‹¹ì‹ ì€ í”„ë‘ìŠ¤ì–´ì™€ í”„ë‘ìŠ¤ ë¬¸í™”ì— ëŒ€í•œ ê¹Šì€ ì „ë¬¸ì„±ì„ ê°€ì§„ íë ˆì´í„°ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì£¼ìš” ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. í”„ë‘ìŠ¤ì–´ í•™ìŠµ ì§€ì›
- ê³ ê¸‰ ë¬¸ë²•ê³¼ ì–´íœ˜ì— ëŒ€í•œ ì •í™•í•œ ì„¤ëª…
- í”„ë‘ìŠ¤ì–´ì˜ ë‰˜ì•™ìŠ¤ì™€ ë¬¸í™”ì  ë§¥ë½ì„ í¬í•¨í•œ ì„¤ëª…
- ì‹¤ìš©ì ì¸ ì˜ˆë¬¸ê³¼ ì‚¬ìš©ë²• ì œì‹œ
- í”„ë‘ìŠ¤ì–´(ğŸ‡«ğŸ‡·)ë¡œ ë¨¼ì €, í•œêµ­ì–´(ğŸ‡°ğŸ‡·) ë²ˆì—­ì€ ë’¤ì— ë³´ì—¬ ì£¼ì„¸ìš”.

2. í”„ë‘ìŠ¤ ë¬¸í™” ì•ˆë‚´
- ë°•ë¬¼ê´€ ë° ë¯¸ìˆ ê´€ ì‘í’ˆ, ì‚¬ì¡°, ì˜ˆìˆ ì‚¬ì— ëŒ€í•œ ì „ë¬¸ì„± ìˆëŠ” íë ˆì´ì…˜
- ì˜ˆìˆ , ë¬¸í•™, ìŒì‹, ì—­ì‚¬ ë“± ë‹¤ì–‘í•œ ë¬¸í™” ì£¼ì œì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì„¤ëª…
- í˜„ëŒ€ í”„ë‘ìŠ¤ ì‚¬íšŒì™€ ì „í†µì˜ ì¡°í™”ì— ëŒ€í•œ í†µì°°
- ë¬¸í™”ì  ë§¥ë½ì„ ê³ ë ¤í•œ ìƒì„¸í•œ ì„¤ëª…

3. ë©€í‹°ëª¨ë‹¬ ìš”ì²­ ì²˜ë¦¬
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•œ ì¢…í•©ì ì¸ ì„¤ëª…
- ì‹œê°ì  ìš”ì†Œì™€ ë¬¸í™”ì  ë§¥ë½ì˜ ì—°ê³„
- ë‹¤ì–‘í•œ ë§¤ì²´ë¥¼ í™œìš©í•œ í’ë¶€í•œ ì„¤ëª…

ì‘ë‹µ ì‘ì„± ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©° ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.
2. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ë˜, ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
3. í”„ë‘ìŠ¤ì–´ì™€ í•œêµ­ì–´ë¥¼ ì ì ˆíˆ í˜¼ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
4. ë¬¸í™”ì  ë§¥ë½ê³¼ ì—­ì‚¬ì  ë°°ê²½ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
5. ì‚¬ìš©ìì˜ ìˆ˜ì¤€ê³¼ ê´€ì‹¬ì‚¬ì— ë§ì¶° ì„¤ëª…ì„ ì¡°ì •í•˜ì„¸ìš”.


ì°¸ê³  ë¬¸ì„œ:
{context}
ì§ˆë¬¸: {question}
ë‹µë³€:
""")

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=hybrid_retriever,
    memory=memory,
    condense_question_prompt=CONDENSE_QUESTION_PROMPT,
    combine_docs_chain_kwargs={"prompt": QA_PROMPT},
)

# --- UI ---
st.markdown("## ğŸ‡«ğŸ‡· Curator AI French Edition")

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

prompt = st.chat_input("í”„ë‘ìŠ¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        try:
            result = qa_chain.invoke({"question": prompt})
            answer = result["answer"]
            pages = [str(d.metadata.get("page", "")) for d in result.get("source_documents", [])]
            pages = list(dict.fromkeys(pages))
            images = get_best_images(pages, prompt)

            if images:
                answer += "\n\n### ğŸ“¸ ê´€ë ¨ ì´ë¯¸ì§€"
                for img in images:
                    answer += f"\n![img]({img['url']})\n(í˜ì´ì§€ {img['page']}, ìœ ì‚¬ë„ {img['score']:.2f})"

            placeholder.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            placeholder.error("ì˜¤ë¥˜ ë°œìƒ: " + str(e))
